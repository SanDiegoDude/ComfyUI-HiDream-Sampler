# Required dependencies
transformers>=4.36.0
diffusers>=0.23.0
torch>=2.0.0
numpy
Pillow
packaging
huggingface_hub
safetensors

# Required for NF4 models
optimum
accelerate

# For 4-bit quantization (required for standard models)
# bitsandbytes

# For faster attention (optional but recommended)
# flash-attn

# ONLY install auto-gptq if modelGPTQ in transformers isn't working
# WARNING: auto-gptq is NOT compatible with Python 3.12 and has issues on Windows
# auto-gptq
